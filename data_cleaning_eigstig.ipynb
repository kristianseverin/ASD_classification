{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook works for loading and reshaping the Eigsti and Nadig datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pylangacq in /home/coder/.local/lib/python3.9/site-packages (0.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.0.0 in /home/coder/.local/lib/python3.9/site-packages (from pylangacq) (2.8.2)\n",
      "Requirement already satisfied: tabulate[widechars]>=0.8.9 in /home/coder/.local/lib/python3.9/site-packages (from pylangacq) (0.9.0)\n",
      "Requirement already satisfied: requests>=2.18.0 in /home/coder/.local/lib/python3.9/site-packages (from pylangacq) (2.28.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/coder/.local/lib/python3.9/site-packages (from python-dateutil>=2.0.0->pylangacq) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/coder/.local/lib/python3.9/site-packages (from requests>=2.18.0->pylangacq) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/coder/.local/lib/python3.9/site-packages (from requests>=2.18.0->pylangacq) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/coder/.local/lib/python3.9/site-packages (from requests>=2.18.0->pylangacq) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/coder/.local/lib/python3.9/site-packages (from requests>=2.18.0->pylangacq) (3.3)\n",
      "Requirement already satisfied: wcwidth in /home/coder/.local/lib/python3.9/site-packages (from tabulate[widechars]>=0.8.9->pylangacq) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE REQUIRED PACKAGES\n",
    "import os\n",
    "os.system('pip install pylangacq')\n",
    "import pylangacq \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITING A FUNCTION FOR LOADING DATA\n",
    "# This function loads each file in the directory individually, and turns it into a dataframe. Then it creates columns for age,\n",
    "# id, and group. Finally, it binds the individual dataframes together.\n",
    "\n",
    "## Accessing single files\n",
    "def dataload(datapath):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for subject in os.listdir(datapath):\n",
    "        #print(subject)\n",
    "        pylang_obj = pylangacq.read_chat(path = datapath, match = subject)\n",
    "        d = pd.DataFrame(pylang_obj.utterances())\n",
    "        d[\"age\"] = pylang_obj.ages(months=True)[0]\n",
    "        d[\"id\"] = pylang_obj.headers()[0]['PID']\n",
    "        d[\"group\"] = pylang_obj.headers()[0]['Participants']['CHI']['group']\n",
    "        df = pd.concat([df, d])\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING THE DATALOAD FUNCTION\n",
    "eigstig = dataload(os.path.join(\"/work\", \"exam\", \"ASD_classification\", \"data\", \"eigstig\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A COLUMN WHERE EACH UTTERANCE IS A STRING\n",
    "# Accessing the word-keys in the nested dicts in the tokens column and appending them to a string in a new tokens column\n",
    "\n",
    "words = \"\"\n",
    "tokens2 = []\n",
    "\n",
    "for row in eigstig['tokens']:\n",
    "    for list in row:\n",
    "        #print(list['word'])\n",
    "        words += list['word'] + \" \"\n",
    "    tokens2.append(words)\n",
    "    words = \"\"\n",
    "\n",
    "eigstig['tokens2'] = tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING THE DF\n",
    "# Dropping unnecessary columns\n",
    "eigstig = eigstig.drop(columns=['tokens'])\n",
    "eigstig = eigstig.drop(columns=['tiers'])\n",
    "eigstig = eigstig.drop(columns=['time_marks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TD' 'DD' 'ASD']\n",
      "['TD' 'ASD']\n"
     ]
    }
   ],
   "source": [
    "# We can read from the description of the datasets on talkbank.org - and see it here - that Eigstig annotated the typically\n",
    "# developing children with TD and Nadig used TYP. For consistency, we will recode all variables that are grouped TYP to TD.\n",
    "# Eigstig also has a group called DD (for developmental delay). These children have developmental delay, but not ASD. This group\n",
    "# will be filtered out.\n",
    "print(eigstig['group'].unique())\n",
    "\n",
    "# Recoding TYP to TD\n",
    "eigstig = eigstig.replace('TYP','TD') # xxx not necessary in the eigstig data\n",
    "\n",
    "# Dropping the rows from the DD group\n",
    "eigstig = eigstig[eigstig.group != 'DD']\n",
    "\n",
    "# Checking the variables\n",
    "print(eigstig['group'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>group</th>\n",
       "      <th>tokens2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INV1</td>\n",
       "      <td>45.633333</td>\n",
       "      <td>11312/a-00032743-1</td>\n",
       "      <td>TD</td>\n",
       "      <td>we've got sort of a bumblebee theme here becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INV1</td>\n",
       "      <td>45.633333</td>\n",
       "      <td>11312/a-00032743-1</td>\n",
       "      <td>TD</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INV1</td>\n",
       "      <td>45.633333</td>\n",
       "      <td>11312/a-00032743-1</td>\n",
       "      <td>TD</td>\n",
       "      <td>mmmm .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INV1</td>\n",
       "      <td>45.633333</td>\n",
       "      <td>11312/a-00032743-1</td>\n",
       "      <td>TD</td>\n",
       "      <td>hm .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INV1</td>\n",
       "      <td>45.633333</td>\n",
       "      <td>11312/a-00032743-1</td>\n",
       "      <td>TD</td>\n",
       "      <td>and you know what ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>INV1</td>\n",
       "      <td>48.400000</td>\n",
       "      <td>11312/a-00032761-1</td>\n",
       "      <td>ASD</td>\n",
       "      <td>yeah , where's the hospital ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>CHI</td>\n",
       "      <td>48.400000</td>\n",
       "      <td>11312/a-00032761-1</td>\n",
       "      <td>ASD</td>\n",
       "      <td>I don't know , it's two blocks here .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>INV1</td>\n",
       "      <td>48.400000</td>\n",
       "      <td>11312/a-00032761-1</td>\n",
       "      <td>ASD</td>\n",
       "      <td>is that the hospital right there ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>CHI</td>\n",
       "      <td>48.400000</td>\n",
       "      <td>11312/a-00032761-1</td>\n",
       "      <td>ASD</td>\n",
       "      <td>and the vinɤs and clean is up the air .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>CHI</td>\n",
       "      <td>48.400000</td>\n",
       "      <td>11312/a-00032761-1</td>\n",
       "      <td>ASD</td>\n",
       "      <td>up the air .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15103 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant        age                  id group  \\\n",
       "0          INV1  45.633333  11312/a-00032743-1    TD   \n",
       "1          INV1  45.633333  11312/a-00032743-1    TD   \n",
       "2          INV1  45.633333  11312/a-00032743-1    TD   \n",
       "3          INV1  45.633333  11312/a-00032743-1    TD   \n",
       "4          INV1  45.633333  11312/a-00032743-1    TD   \n",
       "..          ...        ...                 ...   ...   \n",
       "380        INV1  48.400000  11312/a-00032761-1   ASD   \n",
       "381         CHI  48.400000  11312/a-00032761-1   ASD   \n",
       "382        INV1  48.400000  11312/a-00032761-1   ASD   \n",
       "383         CHI  48.400000  11312/a-00032761-1   ASD   \n",
       "384         CHI  48.400000  11312/a-00032761-1   ASD   \n",
       "\n",
       "                                               tokens2  \n",
       "0    we've got sort of a bumblebee theme here becau...  \n",
       "1                                                   .   \n",
       "2                                              mmmm .   \n",
       "3                                                hm .   \n",
       "4                                 and you know what ?   \n",
       "..                                                 ...  \n",
       "380                     yeah , where's the hospital ?   \n",
       "381             I don't know , it's two blocks here .   \n",
       "382                is that the hospital right there ?   \n",
       "383           and the vinɤs and clean is up the air .   \n",
       "384                                      up the air .   \n",
       "\n",
       "[15103 rows x 5 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigstig\n",
    "\n",
    "# Possible issue: In the Nadig data, there are a lot of rows/utterances that consist only of a punctuation, etc. xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE DF AS A CSV FILE\n",
    "#eigstig.to_csv('df_eigstig.csv', index = True) # Not necessary to save untill cleaning is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INV1' 'INV2' 'MOT' 'CHI' 'FAT' 'INV' 'MOM']\n"
     ]
    }
   ],
   "source": [
    "print(eigstig['participant'].unique())\n",
    "\n",
    "# Dropping the rows that are not participant == CHI\n",
    "eigstig = eigstig[eigstig.participant == 'CHI']\n",
    "\n",
    "# Dropping the column participant (since this is always CHI now)\n",
    "eigstig = eigstig.drop(columns=['participant'])\n",
    "\n",
    "# Dummy coding a diagnosis column\n",
    "eigstig['diagnosis'] = eigstig['group'].replace(\"TD\", 0)\n",
    "eigstig['diagnosis'] = eigstig['diagnosis'].replace(\"ASD\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tokens(df):\n",
    "    \"\"\"This function removes weird and redundant characters and spaces\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    df['tokens2'] = df['tokens2'].str.replace('[^a-zA-Z]', ' ', regex=True)\n",
    "\n",
    "    # Single character removal\n",
    "    df['tokens2'] = df['tokens2'].str.replace(r\"\\s+[a-zA-Z]\\s+\", ' ', regex=True)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    df['tokens2'] = df['tokens2'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "    # Drop spaces created when deleting single period values\n",
    "    df = df[df.tokens2 != ' ']\n",
    "\n",
    "    return df\n",
    "\n",
    "eigstig = preprocess_tokens(eigstig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>group</th>\n",
       "      <th>tokens2</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>45.633333</td>\n",
       "      <td>11312/a-00032743-1</td>\n",
       "      <td>TD</td>\n",
       "      <td>cow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>45.633333</td>\n",
       "      <td>11312/a-00032743-1</td>\n",
       "      <td>TD</td>\n",
       "      <td>tree</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>45.633333</td>\n",
       "      <td>11312/a-00032743-1</td>\n",
       "      <td>TD</td>\n",
       "      <td>bandaid</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>45.633333</td>\n",
       "      <td>11312/a-00032743-1</td>\n",
       "      <td>TD</td>\n",
       "      <td>ow ipep</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>45.633333</td>\n",
       "      <td>11312/a-00032743-1</td>\n",
       "      <td>TD</td>\n",
       "      <td>brush</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>48.400000</td>\n",
       "      <td>11312/a-00032761-1</td>\n",
       "      <td>ASD</td>\n",
       "      <td>and the bunny</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>48.400000</td>\n",
       "      <td>11312/a-00032761-1</td>\n",
       "      <td>ASD</td>\n",
       "      <td>yeah</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>48.400000</td>\n",
       "      <td>11312/a-00032761-1</td>\n",
       "      <td>ASD</td>\n",
       "      <td>I don know it two blocks here</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>48.400000</td>\n",
       "      <td>11312/a-00032761-1</td>\n",
       "      <td>ASD</td>\n",
       "      <td>and the vin and clean is up the air</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>48.400000</td>\n",
       "      <td>11312/a-00032761-1</td>\n",
       "      <td>ASD</td>\n",
       "      <td>up the air</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4869 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age                  id group  \\\n",
       "110  45.633333  11312/a-00032743-1    TD   \n",
       "135  45.633333  11312/a-00032743-1    TD   \n",
       "141  45.633333  11312/a-00032743-1    TD   \n",
       "147  45.633333  11312/a-00032743-1    TD   \n",
       "162  45.633333  11312/a-00032743-1    TD   \n",
       "..         ...                 ...   ...   \n",
       "373  48.400000  11312/a-00032761-1   ASD   \n",
       "377  48.400000  11312/a-00032761-1   ASD   \n",
       "381  48.400000  11312/a-00032761-1   ASD   \n",
       "383  48.400000  11312/a-00032761-1   ASD   \n",
       "384  48.400000  11312/a-00032761-1   ASD   \n",
       "\n",
       "                                  tokens2  diagnosis  \n",
       "110                                  cow           0  \n",
       "135                                 tree           0  \n",
       "141                              bandaid           0  \n",
       "147                              ow ipep           0  \n",
       "162                                brush           0  \n",
       "..                                    ...        ...  \n",
       "373                        and the bunny           1  \n",
       "377                                 yeah           1  \n",
       "381        I don know it two blocks here           1  \n",
       "383  and the vin and clean is up the air           1  \n",
       "384                           up the air           1  \n",
       "\n",
       "[4869 rows x 5 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigstig.to_csv('data/dataframes/data_eigstig.csv', index = False)\n",
    "\n",
    "eigstig\n",
    "\n",
    "# Tokens2 has a lot of columns that have only a \".\". I used the utterances() function to load the data, so it is\n",
    "# only the verbal utterances that are included in this dataset. If we look in the original dataset, we see that\n",
    "# the \".\"'s indicate rows where the child communicated non-verbally, e.g. by nodding, shaking their head, counting\n",
    "# on their fingers, etc. The frequecy of these in the dataset could likely also tell us something about the differ-\n",
    "# ence in language/communication between TD and ASD children, but for our purpose, we will remove these from the \n",
    "# dataset, because we are looking at verbal language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                        32.6\n",
      "id           11312/a-00032742-1\n",
      "group                       ASD\n",
      "tokens2                  Mommy \n",
      "diagnosis                     0\n",
      "dtype: object\n",
      "age                        78.3\n",
      "id           11312/a-00032789-1\n",
      "group                        TD\n",
      "tokens2                    zzz \n",
      "diagnosis                     1\n",
      "dtype: object\n",
      "2.716666666666667\n",
      "6.5249999999999995\n"
     ]
    }
   ],
   "source": [
    "# What is the age range in the data?:\n",
    "print(eigstig.min(axis=0))\n",
    "print(eigstig.max(axis=0))\n",
    "# the age range goes from 32.6 months to 78.3 months, or in years:\n",
    "print(32.6/12)\n",
    "print(78.3/12)\n",
    "# Age range: 2.7-6.5 years old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data up into age groups\n",
    "Since the Eigstig data is collected from children between the ages of 2 and 6 years old, we will split the data into age groups and run the models on data from each separate age group. We do not want to train a model on the full age range, since we are comparing developmental delay of speech between the groups. This means that if we have too wide age groups, then the model may be confused and classify a 6 y.o. autistic child as a 3 y.o. typically developing child.\n",
    "\n",
    "(NB: we may have too little data in the 2- and 6-year-old groups, since the youngest child is 2.7 years old and the oldest child is 6.5 years old).\n",
    "\n",
    "We will have the following age groups:\n",
    "\n",
    "    -   2 year olds: 24-36 months\n",
    "    -   3 year olds: 36-48 months\n",
    "    -   4 year olds: 48-60 months\n",
    "    -   5 year olds: 60-72 months\n",
    "    -   6 year olds: 72-84 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "2435\n",
      "1286\n",
      "729\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "# The age groups in months:\n",
    "eigstig.to_csv('data/dataframes/data_eigstig.csv', index = False)\n",
    "\n",
    "eigstig_age2 = eigstig[(eigstig.age >= 24) & (eigstig.age < 36)]\n",
    "eigstig_age3 = eigstig[(eigstig.age >= 36) & (eigstig.age < 48)]\n",
    "eigstig_age4 = eigstig[(eigstig.age >= 48) & (eigstig.age < 60)]\n",
    "eigstig_age5 = eigstig[(eigstig.age >= 60) & (eigstig.age < 72)]\n",
    "eigstig_age6 = eigstig[(eigstig.age >= 72) & (eigstig.age < 84)]\n",
    "#eigstig_age3to4 = eigstig[(eigstig.age) >= 72 & (eigstig.age < 84)]\n",
    "#eigstig_age5to6 = eigstig[(eigstig.age) >= 72 & (eigstig.age < 84)]\n",
    "\n",
    "eigstig_age2.to_csv('data/dataframes/data_eigstig_age2.csv', index = False)\n",
    "eigstig_age3.to_csv('data/dataframes/data_eigstig_age3.csv', index = False)\n",
    "eigstig_age4.to_csv('data/dataframes/data_eigstig_age4.csv', index = False)\n",
    "eigstig_age5.to_csv('data/dataframes/data_eigstig_age5.csv', index = False)\n",
    "eigstig_age6.to_csv('data/dataframes/data_eigstig_age6.csv', index = False)\n",
    "\n",
    "print(len(eigstig_age2))\n",
    "print(len(eigstig_age3))\n",
    "print(len(eigstig_age4))\n",
    "print(len(eigstig_age5))\n",
    "print(len(eigstig_age6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for LR and NN models (Text and Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    text  label\n",
      "110                 cow       0\n",
      "135                tree       0\n",
      "141             bandaid       0\n",
      "147             ow ipep       0\n",
      "162               brush       0\n",
      "..                   ...    ...\n",
      "314  now we need hammer       0\n",
      "317           your head       0\n",
      "318          oooh grrrr       0\n",
      "320         not hurt no       0\n",
      "322  does that hurts no       0\n",
      "\n",
      "[2435 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe with only text and label features\n",
    "eigstig_age3_text_label = eigstig_age3.drop(columns=['age', 'id', 'group']).rename(columns = {'tokens2':'text', 'diagnosis':'label'})\n",
    "print(eigstig_age3_text_label)\n",
    "eigstig_age3_text_label.to_csv('data/dataframes/data_eigstig_age3_text_label.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data to fit the LogReg and NN classifiers\n",
    "This means that the data must be in a shape of a class, which contains a tuple of three dictionaries: taining, validation, and test data. Inside each subset is a tuple with a dictionary which contains features (list) and number of rows (value). Features is a list which contains text and labels - so for our data, these would be tokens2, age, and group/diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Class with functions - not working\n",
    "# class createDatasetDict:\n",
    "\n",
    "#     def split_data(self, data):\n",
    "#         self.data = data\n",
    "\n",
    "#         # Split dataset into train, test, val (70, 15, 15)\n",
    "#         train, test = train_test_split(df, test_size=0.15)\n",
    "#         train, val = train_test_split(train, test_size=0.15)\n",
    "\n",
    "#         # Turning the split dataframes into dicts\n",
    "#         train = Dataset.from_dict(train)\n",
    "#         val = Dataset.from_dict(val)\n",
    "#         test = Dataset.from_dict(test)\n",
    "\n",
    "#         return(train, val, test)\n",
    "\n",
    "\n",
    "#     def create_dicts(self, train, val, test):\n",
    "#         corpus_dict = datasets.DatasetDict({\n",
    "#             \"train\":self.train, \n",
    "#             \"val\":self.val, \n",
    "#             \"test\":self.test\n",
    "#             })\n",
    "    \n",
    "#         return(corpus_dict)\n",
    "\n",
    "# if __name__==\"__main__\":\n",
    "#     createDatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    1730\n",
      "1     705\n",
      "Name: label, dtype: int64\n",
      "label\n",
      "0    1240\n",
      "1     518\n",
      "Name: label, dtype: int64\n",
      "label\n",
      "0    225\n",
      "1     86\n",
      "Name: label, dtype: int64\n",
      "label\n",
      "0    265\n",
      "1    101\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# For the logreg and nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "data = pd.read_csv(\"/work/exam/ASD_classification/data/dataframes/data_eigstig_age3_text_label.csv\")\n",
    "\n",
    "distribution = data.groupby(['label'])['label'].count()\n",
    "print(distribution)\n",
    "\n",
    "class createDatasetDict:\n",
    "\n",
    "    # Split dataset into train, test, val (70, 15, 15)\n",
    "    #def split_data():\n",
    "    train, test = train_test_split(data, test_size=0.15)\n",
    "    train, val = train_test_split(train, test_size=0.15)\n",
    "    #    return (train, test, val)\n",
    "\n",
    "    distribution = train.groupby(['label'])['label'].count()\n",
    "    print(distribution)\n",
    "    distribution = val.groupby(['label'])['label'].count()\n",
    "    print(distribution)\n",
    "    distribution = test.groupby(['label'])['label'].count()\n",
    "    print(distribution)\n",
    "\n",
    "    # Turning the split dataframes into dicts\n",
    "    train = Dataset.from_dict(train)\n",
    "    val = Dataset.from_dict(val)\n",
    "    test = Dataset.from_dict(test)\n",
    "\n",
    "    corpus_dict = datasets.DatasetDict({\n",
    "        \"train\":train, \n",
    "        \"val\":val, \n",
    "        \"test\":test\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1758\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 311\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 366\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = createDatasetDict()\n",
    "dd = dd.corpus_dict\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dd.corpus_dict)\n",
    "# print(type(dd.corpus_dict))\n",
    "#dd.corpus_dict['train']['tokens2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['TD', 'ASD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the BERT\n",
    "class createDatasetDict:\n",
    "\n",
    "    model_checkpoint = \"distilbert-base-uncased\"\n",
    "\n",
    "    # Split dataset into train, test, val (70, 15, 15)\n",
    "    train, test = train_test_split(data, test_size=0.15)\n",
    "    train, val = train_test_split(train, test_size=0.15)\n",
    "\n",
    "    # Convert to numpy\n",
    "    x_train = train['tokens2'].values[train_idx]\n",
    "    x_test = test['tokens2'].values[test_idx]\n",
    "    x_val = val['tokens2'].values[val_idx]\n",
    "\n",
    "    # Turning the split dataframes into dicts\n",
    "    x_train = Dataset.from_dict(x_train)\n",
    "    x_val = Dataset.from_dict(x_val)\n",
    "    x_test = Dataset.from_dict(x_test)\n",
    "\n",
    "    # Create tokenizer from pretrained model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    corpus_dict = datasets.DatasetDict({\n",
    "        \"train\": [train, x_train], \n",
    "        \"val\": [val, x_val], \n",
    "        \"test\": [test, x_test]\n",
    "        })\n",
    "\n",
    "\n",
    "train_tok = tokenizer(list(x_train), truncation=True, padding=True, max_length=20)\n",
    "val_tok = tokenizer(list(x_val), truncation=True, padding=True, max_length=20)\n",
    "test_tok = tokenizer(list(x_test),  truncation=True, padding=True, max_length=20)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
