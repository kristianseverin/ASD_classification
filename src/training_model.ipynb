{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKIZtBctv8ZS"
      },
      "outputs": [],
      "source": [
        "# Downloading modules \n",
        "!pip install simpletransformers  # force install simpletrransformers in colab-environment\n",
        "!pip install wandb # force install wandb in colab-environment\n",
        "\n",
        "# Importing modules\n",
        "import pandas as pd\n",
        "import simpletransformers\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "import torch\n",
        "from torch.nn.modules.activation import Threshold\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import wandb\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login  # using this requires a Weights & Biases account: https://wandb.ai/login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wl7dWy4wsFc",
        "outputId": "208b2b6f-ec74-4186-9ed3-d6a1ab965112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount colab to Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3aqDn1rxAEn",
        "outputId": "88434407-ec11-40d2-e449-183f02cac3d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/ModelFolder/data_eigstig_text_label.csv\")  # must be set to the exact path of the data"
      ],
      "metadata": {
        "id": "NuwuyQ-zxPWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into train and test\n",
        "train, test = train_test_split(data, test_size=0.1)\n",
        "\n",
        "# train data to use for training and test (splitting training into train and val later on)\n",
        "train_data = train  # this is used in 'training_model_weightdecay()'\n",
        "test_data =  test\n",
        "\n",
        "# write data to csv, so we can always evaluate the model later\n",
        "test_data.to_csv('ModelAllDataWeightDecay.csv')  # a user path can be inserted into the str argument if wanted"
      ],
      "metadata": {
        "id": "ySkLkZpWedSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set logging information \n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "def training_model_weightdecay():\n",
        "  '''\n",
        "  This function initializes a Weights & Biases project called \"ModelAllDataWeightDecay\"\n",
        "  It also sets up 5 folds for cross-validation. \n",
        "  Model arguments are set for a \"bert-base-uncased\" model that uses GPU.\n",
        "  The model performs training on a training set and appends results to a list called \"results\"\n",
        "\n",
        "  '''\n",
        "    # intialize new wandb project\n",
        "    wandb.init(project = \"ModelAllDataWeightDecay\")\n",
        "\n",
        "    # set k_fold-specifics\n",
        "    n = 5\n",
        "    seed = 43\n",
        "    kf = KFold(n_splits=n, random_state = seed, shuffle=True)\n",
        "\n",
        "    # model arguments\n",
        "    model_args = ClassificationArgs()\n",
        "    model_args.output_dir = '/content/drive/MyDrive/ASD_second_account/Model_weightDecay'  # change to desired output directory \n",
        "    model_args.num_train_epochs = 20\n",
        "    model_args.learning_rate = 0.00001\n",
        "    model_args.train_batch_size = 32\n",
        "    model_args.overwrite_output_dir = True\n",
        "    model_args.evaluate_during_training = True\n",
        "    model_args.use_multiprocessing = True\n",
        "    model_args.save_best_model = True\n",
        "    model_args.weigth_decay = 0.1\n",
        "    model_args.wandb_project = \"ModelAllDataWeightDecay\"\n",
        "\n",
        "    # Defining model using k-folds\n",
        "    results = [] \n",
        "    for train_index, val_index in kf.split(train_data):\n",
        "      # splitting Dataframe (dataset not included)\n",
        "      train_df = train_data.iloc[train_index]\n",
        "      val_df = train_data.iloc[val_index]\n",
        "      # Defining Model\n",
        "      model = ClassificationModel('bert', 'bert-base-uncased', use_cuda=True, args= model_args, num_labels = 2)\n",
        "      # train the model\n",
        "      model.train_model(train_df, eval_df = val_df, acc = sklearn.metrics.accuracy_score)\n",
        "      # validate the model\n",
        "      result, model_outputs, wrong_predictions = model.eval_model(val_df, acc = sklearn.metrics.accuracy_score) # , acc = accuracy_score\n",
        "      print(result['acc'])\n",
        "      # append model score\n",
        "      results.append(result['acc'])\n",
        "\n",
        "    print(\"results\",results)\n",
        "    print(f\"Mean-Precision: {sum(results) / len(results)}\")"
      ],
      "metadata": {
        "id": "X9Wks25dz6gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model with weight decay\n",
        "training_model_weightdecay()"
      ],
      "metadata": {
        "id": "MHsGTvUc20Fy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}